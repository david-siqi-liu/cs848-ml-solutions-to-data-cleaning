Paper Title: Detecting Data Errors: Where are we and what needs to be done?

1. Overall Rating
[ ] Strong Accept
[x] Accept
[ ] Weak Accept
[ ] Weak Reject
[ ] Reject


2. Primary Focus
[ ] Theory
[x] Algorithmic Improvement
[ ] Systems
[ ] Implementation
[x] Empirical Evaluation


3. Novelty
[ ] High
[x] Medium
[ ] Low


4. Technical Depth
[ ] High
[x] Medium
[ ] Low


5. Presentation
[ ] Very good
[x] Adequate
[ ] Not good


6. Reviewer Confidence
[x] High
[ ] Medium
[ ] Low


7. Justification of Decision (one paragraph)

This paper serves as a general overview of the existing, commercially available algorithms (as of 2016), which errors they aim to tackle, how to combine them to achieve maximum coverage, and which errors are still not detectable by any algorithm. While the data sets this paper used covered a wide range of real-life applications, the algorithms that this paper covered, unfortunately do not have a lot of overlap. In fact, among the four errors that this paper investigated, only "pattern violations" have more than one algorithm that focuses on it. This does not provide a very good comparison benchmark. It is worth noting that this paper was published in 2016, and a lot of algorithms have been introduced since then (e.g. Uni-Detect, Raha). Overall, it was interesting too see how different combinations of algorithms could lead to different levels of error detection coverage, and that the existing algorithms (as of 2016) were well below the desirable coverage.


8. Detailed comments (please number each comment) 

# Notes

- Recall upper bound
- F-Measure $= 2(R \times P) / (R + P)$, combining both Precision and Recall
- Union All v.s. Min-K (at least k tools must agree)
- Cost to run each tool on a dataset $P \times V > (P + N) \times C$
  - P = correctly detected errors
  - N = false positive
  - V = value of detection
  - C = cost of human
  - Set $\frac{C}{V} = \sigma$ such that any tool below this threshold does not get run
- 